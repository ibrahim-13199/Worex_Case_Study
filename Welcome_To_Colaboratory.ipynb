{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ibrahim-13199/Worex_Case_Study/blob/main/Welcome_To_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuer Spark\n"
      ],
      "metadata": {
        "id": "V7IOg5rJ0J9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://bitbucket.org/habedi/datasets/raw/b6769c4664e7ff68b001e2f43bc517888cbe3642/spark/spark-3.0.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.2-bin-hadoop2.7.tgz\n",
        "!rm -rf spark-3.0.2-bin-hadoop2.7.tgz*"
      ],
      "metadata": {
        "id": "hCGdrDaakgGE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark py4j\n",
        "!pip -q install findspark pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9biBB5t0XkB",
        "outputId": "e43f83c4-9074-4dbb-db29-c7b65449009e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=f70fe2426e5dff6379a6c11ee3bb25d923f24a7112f7199667cd6222ac062ea4\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.2-bin-hadoop2.7\"\n",
        "os.environ[\"HADOOP_HOME\"] = os.environ[\"SPARK_HOME\"]\n",
        "\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"jupyter\"\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"] = \"notebook\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--master local[*] pyspark-shell\""
      ],
      "metadata": {
        "id": "sYtc5fv-5TF7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export PYSPARK_SUBMIT_ARGS=\"--master local[*] pyspark-shell\"\n",
        "!export PYSPARK_DRIVER_PYTHON=jupyter\n",
        "!export PYSPARK_DRIVER_PYTHON_OPTS=notebook"
      ],
      "metadata": {
        "id": "ql7FX6Sj5lUI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "DR3pLztdiIpq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf\n",
        "from pyspark.context import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "data = sc.textFile(\"/content/insurance.csv\")"
      ],
      "metadata": {
        "id": "-vg5szhY50Ux"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-nQofrVqxCn",
        "outputId": "8ad5e14f-69e2-46bb-b6d5-953bd6499504"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "/content/insurance.csv MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPBWZceYcANu",
        "outputId": "f2e03c1d-cbff-4f5a-ea37-c3c9c4869aa5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['age,sex,bmi,children,smoker,region,charges',\n",
              " '19,female,27.9,0,yes,southwest,16884.924',\n",
              " '18,male,33.77,1,no,southeast,1725.5523',\n",
              " '28,male,33,3,no,southeast,4449.462',\n",
              " '33,male,22.705,0,no,northwest,21984.47061',\n",
              " '32,male,28.88,0,no,northwest,3866.8552',\n",
              " '31,female,25.74,0,no,southeast,3756.6216',\n",
              " '46,female,33.44,1,no,southeast,8240.5896',\n",
              " '37,female,27.74,3,no,northwest,7281.5056',\n",
              " '37,male,29.83,2,no,northeast,6406.4107']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Impelementing DataFrame"
      ],
      "metadata": {
        "id": "HSgGQaAdDsPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession , functions as f\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Task\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "_GSip2xsMKY4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"/content/insurance.csv\",header=True)\n",
        "df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghlnEHN1N85x",
        "outputId": "be597c50-ba96-4547-e9f3-b94642f5c90e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+--------+------+---------+-----------+\n",
            "|age|   sex|   bmi|children|smoker|   region|    charges|\n",
            "+---+------+------+--------+------+---------+-----------+\n",
            "| 19|female|  27.9|       0|   yes|southwest|  16884.924|\n",
            "| 18|  male| 33.77|       1|    no|southeast|  1725.5523|\n",
            "| 28|  male|    33|       3|    no|southeast|   4449.462|\n",
            "| 33|  male|22.705|       0|    no|northwest|21984.47061|\n",
            "| 32|  male| 28.88|       0|    no|northwest|  3866.8552|\n",
            "| 31|female| 25.74|       0|    no|southeast|  3756.6216|\n",
            "| 46|female| 33.44|       1|    no|southeast|  8240.5896|\n",
            "| 37|female| 27.74|       3|    no|northwest|  7281.5056|\n",
            "| 37|  male| 29.83|       2|    no|northeast|  6406.4107|\n",
            "| 60|female| 25.84|       0|    no|northwest|28923.13692|\n",
            "+---+------+------+--------+------+---------+-----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IEyMet7PUo_",
        "outputId": "efe1a856-f7d0-4256-c171-a18b2bee2252"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- age: string (nullable = true)\n",
            " |-- sex: string (nullable = true)\n",
            " |-- bmi: string (nullable = true)\n",
            " |-- children: string (nullable = true)\n",
            " |-- smoker: string (nullable = true)\n",
            " |-- region: string (nullable = true)\n",
            " |-- charges: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only the \"name\" column\n",
        "df.select(\"region\").show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJvYdHeHP89z",
        "outputId": "044aae0e-5bcb-403d-825b-9c101683b06c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|   region|\n",
            "+---------+\n",
            "|southwest|\n",
            "|southeast|\n",
            "|southeast|\n",
            "|northwest|\n",
            "|northwest|\n",
            "|southeast|\n",
            "|southeast|\n",
            "|northwest|\n",
            "|northeast|\n",
            "|northwest|\n",
            "+---------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df['age'] > 21).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eny-jm-tQIfc",
        "outputId": "87008a2e-8bb1-4e44-fe20-35863be95bb1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+--------+------+---------+-----------+\n",
            "|age|   sex|   bmi|children|smoker|   region|    charges|\n",
            "+---+------+------+--------+------+---------+-----------+\n",
            "| 28|  male|    33|       3|    no|southeast|   4449.462|\n",
            "| 33|  male|22.705|       0|    no|northwest|21984.47061|\n",
            "| 32|  male| 28.88|       0|    no|northwest|  3866.8552|\n",
            "| 31|female| 25.74|       0|    no|southeast|  3756.6216|\n",
            "| 46|female| 33.44|       1|    no|southeast|  8240.5896|\n",
            "| 37|female| 27.74|       3|    no|northwest|  7281.5056|\n",
            "| 37|  male| 29.83|       2|    no|northeast|  6406.4107|\n",
            "| 60|female| 25.84|       0|    no|northwest|28923.13692|\n",
            "| 25|  male| 26.22|       0|    no|northeast|  2721.3208|\n",
            "| 62|female| 26.29|       0|   yes|southeast| 27808.7251|\n",
            "| 23|  male|  34.4|       0|    no|southwest|   1826.843|\n",
            "| 56|female| 39.82|       0|    no|southeast| 11090.7178|\n",
            "| 27|  male| 42.13|       0|   yes|southeast| 39611.7577|\n",
            "| 52|female| 30.78|       1|    no|northeast| 10797.3362|\n",
            "| 23|  male|23.845|       0|    no|northeast| 2395.17155|\n",
            "| 56|  male|  40.3|       0|    no|southwest|  10602.385|\n",
            "| 30|  male|  35.3|       0|   yes|southwest|  36837.467|\n",
            "| 60|female|36.005|       0|    no|northeast|13228.84695|\n",
            "| 30|female|  32.4|       1|    no|southwest|   4149.736|\n",
            "| 34|female| 31.92|       1|   yes|northeast| 37701.8768|\n",
            "+---+------+------+--------+------+---------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Group By Regoins\n",
        "df.groupBy(\"region\").count().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQRYysicQQZp",
        "outputId": "cafbe78a-5f88-4e0f-a916-f3de49a9f182"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+\n",
            "|   region|count|\n",
            "+---------+-----+\n",
            "|northwest|  325|\n",
            "|southeast|  364|\n",
            "|northeast|  324|\n",
            "|southwest|  325|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3FQ0JizQhT3",
        "outputId": "27106ccb-0686-499a-b3a5-e83ae8915aeb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1339"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Total Number of BMI\n",
        "df.select(f.sum('bmi').alias(\"Total Num Of Bmi\")).show()\n",
        "# df.select(f.avg('bmi').alias(\"Avearge of BMI\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ag9M5sTTkU8",
        "outputId": "fa17bdf2-213f-4805-db04-76de17bf572f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+\n",
            "|  Total Num Of Bmi|\n",
            "+------------------+\n",
            "|41027.624999999985|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Number of BMI\n",
        "df.select(f.mean('bmi').alias(\"Total_Bmi\")).show()\n",
        "# Max Age\n",
        "df.select(f.max('age').alias(\"Max Age\")).show()\n",
        "# Mini Age\n",
        "df.select(f.min('age').alias(\"Mini Age\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqT54-HLVmjw",
        "outputId": "edf215c6-e6ff-45e3-e02d-4d78b2c48776"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+\n",
            "|         Total_Bmi|\n",
            "+------------------+\n",
            "|30.663396860986538|\n",
            "+------------------+\n",
            "\n",
            "+-------+\n",
            "|Max Age|\n",
            "+-------+\n",
            "|     64|\n",
            "+-------+\n",
            "\n",
            "+--------+\n",
            "|Mini Age|\n",
            "+--------+\n",
            "|      18|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the Total Sum Of all AGEs in each Regoin\n",
        "from pyspark.sql.types import *\n",
        "df.withColumn(\"age\",df.age.cast(IntegerType())).groupBy(\"region\").agg(f.sum('age').alias(\"Total Ages in Each Regoin\")).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd7aSgdpYRlb",
        "outputId": "d8c00391-b376-4cef-c8c8-cbfabe388300"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------------+\n",
            "|   region|Total Ages in Each Regoin|\n",
            "+---------+-------------------------+\n",
            "|northwest|                    12739|\n",
            "|southeast|                    14174|\n",
            "|northeast|                    12723|\n",
            "|southwest|                    12823|\n",
            "+---------+-------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#agg to can use alias()\n",
        "# Get the Max Num Of all AGEs in each Regoin\n",
        "from pyspark.sql.types import *\n",
        "df.withColumn(\"bmi\",df.bmi.cast(IntegerType())).groupBy(\"region\").agg(f.max('bmi').alias(\"Max bmi in Each Regoin\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkso9zAbhQi6",
        "outputId": "e6f2794f-dd40-4d56-976a-1195909cb6da"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------------------+\n",
            "|   region|Max bmi in Each Regoin|\n",
            "+---------+----------------------+\n",
            "|northwest|                    42|\n",
            "|southeast|                    53|\n",
            "|northeast|                    48|\n",
            "|southwest|                    47|\n",
            "+---------+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Rdd functions"
      ],
      "metadata": {
        "id": "FUqZskbr9uVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#to implement the persist rdd\n",
        "dataLengths = data.map(lambda s: len(s))  #this is transformation\n",
        "totalLength = dataLengths.reduce(lambda a, b: a + b) #action\n",
        "print(dataLengths)"
      ],
      "metadata": {
        "id": "slwEijtXQQ24",
        "outputId": "0007de5d-c13f-47eb-f90b-70ad7eef5267",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PythonRDD[134] at RDD at PythonRDD.scala:53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lineLengths.persist()  #if we need it again so do it in persist\n",
        "#lineLengths.unpersist()"
      ],
      "metadata": {
        "id": "4r4eCjS5UoKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#implement distinct func\n",
        "\n",
        "result=data.distinct()\n",
        "result.count()"
      ],
      "metadata": {
        "id": "mtstRDVkWEG_",
        "outputId": "57f98e64-e818-40b3-b875-4e1a09d62847",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1338"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Accumulator\n",
        "\n",
        "# Identify the values that need to be collected or aggregated\n",
        "total_count = spark.sparkContext.accumulator(0)\n",
        "\n",
        "# Update the accumulator within Spark transformations or actions\n",
        "df.foreach(lambda row: total_count.add(1))\n",
        "\n",
        "# Retrieve the accumulated value at the driver node\n",
        "accumulated_value = total_count.value\n",
        "\n",
        "print(accumulated_value)\n",
        ""
      ],
      "metadata": {
        "id": "KNhtdBYtlA1v",
        "outputId": "95edff36-2f70-4a8b-efd4-92cc2e6ad4cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ZJauDQTlEXA",
        "outputId": "c13e4431-b29b-4fac-8f50-e80d4e90292a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        }
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/broadcast.py\", line 111, in dump\n",
            "    pickle.dump(value, f, pickle_protocol)\n",
            "  File \"/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/rdd.py\", line 278, in __getnewargs__\n",
            "    raise Exception(\n",
            "Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PicklingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/broadcast.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, value, f)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickleError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m__getnewargs__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;31m# This method is called when attempting to pickle an RDD, which is always an error:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         raise Exception(\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0;34m\"It appears that you are attempting to broadcast an RDD or reference an RDD from an \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-119-be52515486e7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mwords_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0melem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36mbroadcast\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0mbe\u001b[0m \u001b[0msent\u001b[0m \u001b[0mto\u001b[0m \u001b[0meach\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0monly\u001b[0m \u001b[0monce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \"\"\"\n\u001b[0;32m--> 900\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pickled_broadcast_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccum_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/broadcast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sc, value, pickle_registry, path, sock_file)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;31m# no encryption, we can just write pickled data directly to the file from python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mbroadcast_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encryption_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_broadcast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitTillDataReceived\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/broadcast.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, value, f)\u001b[0m\n\u001b[1;32m    116\u001b[0m                   \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_exception_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mprint_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPicklingError\u001b[0m: Could not serialize broadcast: Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}